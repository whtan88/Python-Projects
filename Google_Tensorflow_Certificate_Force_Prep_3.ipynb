{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPcan5okLbe4tPmMeU++Qew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whtan88/Python-Projects/blob/master/Google_Tensorflow_Certificate_Force_Prep_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Concepts:\n",
        "\n",
        "\n",
        "*   Build natural language processing systems\n",
        "*   Prepare text to use the models\n",
        "*   Use word embeddings\n",
        "*   Train LSTMs or GRUs on existing text to generate text (such as songs and poetry)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HWc19u2kExEH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EjrwOduhErUQ"
      },
      "outputs": [],
      "source": [
        "#import libraries to use\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, GRU\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#important to use for bypassing cert error issues\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the dataset. Using Shakespeare's sonnet\n",
        "!pip install gdown==5.1.0\n",
        "!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxYfwwYgpMnj",
        "outputId": "b49c3f0c-6e9e-49fa-8c2f-415e3e6abc0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gdown==5.1.0\n",
            "  Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==5.1.0) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==5.1.0) (3.13.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==5.1.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown==5.1.0) (4.66.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==5.1.0) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==5.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==5.1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==5.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==5.1.0) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==5.1.0) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.7.3\n",
            "    Uninstalling gdown-4.7.3:\n",
            "      Successfully uninstalled gdown-4.7.3\n",
            "Successfully installed gdown-5.1.0\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n",
            "To: /content/sonnets.txt\n",
            "100% 93.6k/93.6k [00:00<00:00, 5.17MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define path for file with sonnets\n",
        "SONNETS_FILE = './sonnets.txt'\n",
        "\n",
        "#Read the data\n",
        "with open('./sonnets.txt') as f:\n",
        "    data = f.read()\n",
        "\n",
        "#Convert to lower case and save as a list\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "print(f\"There are {len(corpus)} lines of sonnets\\n\")\n",
        "\n",
        "#Check the average number of words per line\n",
        "avg_words_line = []\n",
        "for line in corpus:\n",
        "  line = str(line)\n",
        "  count_words = [word for word in line.split()]\n",
        "  avg_words_line.append(len(count_words))\n",
        "\n",
        "avg_words_line = int(np.average(avg_words_line))\n",
        "print(f'\\nThe average number of words per line is : {avg_words_line}\\n')\n",
        "\n",
        "#Inspect the first sonnet\n",
        "print(f\"The first sonnet look like this:\\n\")\n",
        "for i in range(14):\n",
        "  print(corpus[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLYtsW229D4e",
        "outputId": "b5db71cd-26b0-438c-fbec-5e453e2e6804"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2159 lines of sonnets\n",
            "\n",
            "\n",
            "The average number of words per line is : 8\n",
            "\n",
            "The first sonnet look like this:\n",
            "\n",
            "from fairest creatures we desire increase,\n",
            "that thereby beauty's rose might never die,\n",
            "but as the riper should by time decease,\n",
            "his tender heir might bear his memory:\n",
            "but thou, contracted to thine own bright eyes,\n",
            "feed'st thy light'st flame with self-substantial fuel,\n",
            "making a famine where abundance lies,\n",
            "thyself thy foe, to thy sweet self too cruel.\n",
            "thou that art now the world's fresh ornament\n",
            "and only herald to the gaudy spring,\n",
            "within thine own bud buriest thy content\n",
            "and, tender churl, makest waste in niggarding.\n",
            "pity the world, or else this glutton be,\n",
            "to eat the world's due, by the grave and thee.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "#Inspecting the sequence of the first line\n",
        "tokenizer.texts_to_sequences([corpus[0]])[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCChPKcp-dZF",
        "outputId": "13f930ca-159f-4b9a-feff-6f76ee2ad441"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[34, 417, 877, 166, 213, 517]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to generate n-gram sequence\n",
        "def n_gram_seqs(corpus, tokenizer):\n",
        "    input_sequences = []\n",
        "\n",
        "    for line in corpus:\n",
        "      token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "      for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences\n",
        "\n",
        "#Inspect the results of the n-grams sequence function\n",
        "first_example_sequence = n_gram_seqs([corpus[0]], tokenizer)\n",
        "print(\"n_gram sequences for first example look like this:\\n\")\n",
        "first_example_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFkFhqcOAJdB",
        "outputId": "c436fdff-f8f9-44da-96d0-9d398c28c65a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for first example look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417],\n",
              " [34, 417, 877],\n",
              " [34, 417, 877, 166],\n",
              " [34, 417, 877, 166, 213],\n",
              " [34, 417, 877, 166, 213, 517]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply the n-grams sequence function to the whole corpus\n",
        "input_sequences = n_gram_seqs(corpus, tokenizer)\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "print(f\"n_grams of input_sequences have length: {len(input_sequences)}\")\n",
        "print(f\"maximum length of sequences is: {max_sequence_len}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeHumcnmAu_O",
        "outputId": "d6f57739-cd22-491e-ab11-bb4808e27fc3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_grams of input_sequences have length: 15462\n",
            "maximum length of sequences is: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create padding sequence function\n",
        "def pad_seqs(input_sequences, maxlen):\n",
        "    padded_sequences = np.array(pad_sequences(input_sequences, maxlen=maxlen, padding='pre'))\n",
        "    return padded_sequences\n",
        "\n",
        "#Inspect the results of the padding sequence function\n",
        "# Test your function with the n_grams_seq of the first example\n",
        "first_padded_seq = pad_seqs(first_example_sequence, max([len(x) for x in first_example_sequence]))\n",
        "first_padded_seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrKTL1GFBtSD",
        "outputId": "f234c77b-27a3-4b40-cda9-fbe014bf1d43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,  34, 417],\n",
              "       [  0,   0,   0,  34, 417, 877],\n",
              "       [  0,   0,  34, 417, 877, 166],\n",
              "       [  0,  34, 417, 877, 166, 213],\n",
              "       [ 34, 417, 877, 166, 213, 517]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pad the entire corpus\n",
        "input_sequences = pad_seqs(input_sequences, max_sequence_len)\n",
        "print(f\"padded corpus has shape: {input_sequences.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fab9LoifBtZW",
        "outputId": "421fab16-40db-4afb-be94-ba389ff361d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padded corpus has shape: (15462, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a function to split the data into features and labels\n",
        "def features_and_labels(input_sequences, total_words):\n",
        "    features = input_sequences[:,:-1]\n",
        "    labels = input_sequences[:,-1]\n",
        "    one_hot_labels = to_categorical(labels, num_classes=total_words)\n",
        "    return features, one_hot_labels\n",
        "\n",
        "#Test the function with a sample padded n_grams_seq\n",
        "first_features, first_labels = features_and_labels(first_padded_seq, total_words)\n",
        "\n",
        "print(f\"labels have shape: {first_labels.shape}\")\n",
        "print(\"\\nfeatures look like this:\\n\")\n",
        "first_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1cCVr_BBtjf",
        "outputId": "51e871bb-1ff2-4c91-e929-6c1811f7ce22"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels have shape: (5, 3211)\n",
            "\n",
            "features look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,  34],\n",
              "       [  0,   0,   0,  34, 417],\n",
              "       [  0,   0,  34, 417, 877],\n",
              "       [  0,  34, 417, 877, 166],\n",
              "       [ 34, 417, 877, 166, 213]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the entire corpus\n",
        "features, labels = features_and_labels(input_sequences, total_words)\n",
        "print(f\"features have shape: {features.shape}\")\n",
        "print(f\"labels have shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVjORMGYEE_x",
        "outputId": "3113fb7d-14e6-433d-ebd0-61a5cae9c504"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features have shape: (15462, 10)\n",
            "labels have shape: (15462, 3211)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the model and train it\n",
        "def create_model(total_words, max_sequence_len):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "    model.add(Bidirectional(LSTM(2000,return_sequences=True)))\n",
        "    model.add(GRU(1000))\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = create_model(total_words, max_sequence_len)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(features, labels, epochs=30, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmDggPqWELV_",
        "outputId": "d12c7c84-8792-47fe-9ccb-994cf69fe43f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "484/484 [==============================] - 39s 68ms/step - loss: 6.7833 - accuracy: 0.0238\n",
            "Epoch 2/30\n",
            "484/484 [==============================] - 31s 65ms/step - loss: 6.3954 - accuracy: 0.0367\n",
            "Epoch 3/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 6.1541 - accuracy: 0.0445\n",
            "Epoch 4/30\n",
            "484/484 [==============================] - 31s 63ms/step - loss: 5.8982 - accuracy: 0.0556\n",
            "Epoch 5/30\n",
            "484/484 [==============================] - 31s 63ms/step - loss: 5.6509 - accuracy: 0.0689\n",
            "Epoch 6/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 5.3907 - accuracy: 0.0819\n",
            "Epoch 7/30\n",
            "484/484 [==============================] - 31s 63ms/step - loss: 5.1177 - accuracy: 0.1008\n",
            "Epoch 8/30\n",
            "484/484 [==============================] - 30s 63ms/step - loss: 4.8378 - accuracy: 0.1236\n",
            "Epoch 9/30\n",
            "484/484 [==============================] - 30s 63ms/step - loss: 4.5328 - accuracy: 0.1505\n",
            "Epoch 10/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 4.2176 - accuracy: 0.1771\n",
            "Epoch 11/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 3.8589 - accuracy: 0.2194\n",
            "Epoch 12/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 3.4742 - accuracy: 0.2637\n",
            "Epoch 13/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 3.0695 - accuracy: 0.3236\n",
            "Epoch 14/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 2.6679 - accuracy: 0.3930\n",
            "Epoch 15/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 2.2493 - accuracy: 0.4783\n",
            "Epoch 16/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 1.8613 - accuracy: 0.5614\n",
            "Epoch 17/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 1.5427 - accuracy: 0.6309\n",
            "Epoch 18/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 1.2677 - accuracy: 0.6955\n",
            "Epoch 19/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 1.0716 - accuracy: 0.7400\n",
            "Epoch 20/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 0.9325 - accuracy: 0.7730\n",
            "Epoch 21/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 0.8582 - accuracy: 0.7875\n",
            "Epoch 22/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 0.8060 - accuracy: 0.7991\n",
            "Epoch 23/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 0.7919 - accuracy: 0.8031\n",
            "Epoch 24/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 0.7699 - accuracy: 0.8055\n",
            "Epoch 25/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 0.7193 - accuracy: 0.8192\n",
            "Epoch 26/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 0.6840 - accuracy: 0.8261\n",
            "Epoch 27/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 0.6823 - accuracy: 0.8247\n",
            "Epoch 28/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 0.6832 - accuracy: 0.8241\n",
            "Epoch 29/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 0.6866 - accuracy: 0.8196\n",
            "Epoch 30/30\n",
            "484/484 [==============================] - 30s 62ms/step - loss: 0.6629 - accuracy: 0.8262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \" Thy warm loving embrace i seek,\\n\"\n",
        "sonnet_line = 13\n",
        "next_words = avg_words_line\n",
        "\n",
        "for i in range(sonnet_line):\n",
        "  for j in range(next_words):\n",
        "      # Convert the text into sequences\n",
        "      token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "      # Pad the sequences\n",
        "      token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "      # Get the probabilities of predicting a word\n",
        "      predicted = model.predict(token_list, verbose=0)\n",
        "      # Choose the next word based on the maximum probability\n",
        "      predicted = np.argmax(predicted, axis=-1).item()\n",
        "      # Get the actual word from the word index\n",
        "      output_word = tokenizer.index_word[predicted]\n",
        "      # Append to the current text\n",
        "      seed_text += \" \" + output_word\n",
        "      if i ==sonnet_line-1:\n",
        "        seed_text = seed_text + \".\"\n",
        "      elif j == next_words-1:\n",
        "        seed_text = seed_text + \",\\n\"\n",
        "seed_text = str(seed_text)\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPJSdRvXIWho",
        "outputId": "fe196a31-ae10-4a0a-967b-16626c8f0c96"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Thy warm loving embrace i seek,\n",
            " which doth lie to my friend one happy,\n",
            " grow sad glory be old burn'd hold nature's,\n",
            " woe tell o'er store me disgrace and some,\n",
            " winters cold decay out even even in hue,\n",
            " night grow mother woe now lend me held,\n",
            " quite back again assured eyes can see such,\n",
            " dearest spite of thine eyes dote women's pleasure,\n",
            " had stol'n of both men prove scorn to,\n",
            " change my desire of her scope can sing,\n",
            " even by away eyes see not some stand,\n",
            " can be broken told i none buried age,\n",
            " would cover both one night grow have done,\n",
            " so. deem'd. another. pleasure. change. you. be. a.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dFwkijy0UeOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}